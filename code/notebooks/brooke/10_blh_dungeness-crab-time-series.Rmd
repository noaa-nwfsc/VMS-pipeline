---
title: "10_blh_dungeness-crab-time-series"
author: "Brooke Hawkins"
date: "`r Sys.Date()`"
output: html_document
---

```{r, include=FALSE}
# start timer
start_timer <- proc.time()
```

## Focus: Time series

* Landings and revenue time series from fish tickets for main deck

```{r setup, include=FALSE}
# import libraries
library(tidyverse)
library(here)
library(fredr)
```

```{r create-output-directories}
# make hidncast output directory, if doesn't yet exist
output_dir <- here('Confidential', 'hindcast_output')
if (!dir.exists(output_dir)) dir.create(output_dir)

# create hindcast output subdirectory name based on system date
output_subdir_name <- paste0("hindcast_output_", Sys.Date())

# make hindcast output subdirectory, if doesn't yet exist
output_subdir <- here('Confidential', 'hindcast_output', output_subdir_name)
if (!dir.exists(output_subdir)) dir.create(output_subdir)

# make tables, figures, and maps subdirectories
for (temp_name in c('tables', 'figures', 'maps')) {
  temp_subdir <- here('Confidential', 'hindcast_output', output_subdir_name, temp_name)
  if (!dir.exists(temp_subdir)) dir.create(temp_subdir)
}
rm(temp_name, temp_subdir)
```

## Load data

1. Load the cleaned fish ticket data for specified years, which will be used to plot revenue and landings across time.

Pre-requisite: Run the pipeline steps 1-6 (including interpolation) for calendar years 2011-2023 for DCRB.

```{r load-data}
# choose years of data to load
load_years <- 2011:2023

# load fish ticket data before it was joined to VMS data, used to check VMS representativeness
ticket_df <- purrr::map(load_years, function(ly) {
  read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'fish_tickets', paste0('fishtix_vlengths_withFTID_', ly, '.rds')))
}) %>% bind_rows()
```

2. Load inflation adjustment factors from FRED.

Adjust revenue for inflation using Fred GDP data, adapted from R code by Erin Steiner. This product uses the FREDÂ® API but is not endorsed or certified by the Federal Reserve Bank of St. Louis.

Pre-requisite: Create a FRED account and API key. See https://fred.stlouisfed.org/docs/api/api_key.html.

```{r inflation-adjustment}
# insert your FRED API key
fredr_set_key('')

# download the quarterly inflation adjustments from Fred from 1985 to present
fred_gdpdefl <- fredr(
    series_id = "GDPDEF", 
    observation_start = as.Date(paste0(min(load_years), "-01-01"))
  )

# generate the mean annual deflators based on the quarterly values
gdp_defl <- mutate(fred_gdpdefl, year = year(date)) %>%
  group_by(year) %>%
  summarize(defl = mean(value), .groups = 'drop') %>%
  mutate(inflation_adjustment_factor = defl / defl[year == max(year)]) %>%
  select(year, inflation_adjustment_factor)

# write inflation adjustment factors
write.csv(file = here("Confidential", "hindcast_output", output_subdir_name, "tables", "inflation_adjustment.csv"), x = gdp_defl)

# plot inflation adjustment factors
gdp_defl %>% ggplot(aes(x = as.factor(year), y = inflation_adjustment_factor)) + geom_point()
ggsave(here("Confidential", "hindcast_output", output_subdir_name, "figures", "inflation_adjustment.png"))

# clean up
rm(fred_gdpdefl)
```

The reference year for inflation adjustment is `r max(gdp_defl$year)`.

## Transform data

Transform the joined, cleaned, interpolated VMS and fish ticket data:

1. Filter for dungeness crab related records.
2. Add temporal columns.
3. Adjust revenue for inflation with data from FRED.

Some commonly used acronyms for variable naming in the code include:

* `dcrb` dungeness crab
* `rev` revenue
* `lbs` landings
* `VMS` vessel monitoring system
* `afi` adjusted for inflation

```{r define-filters}
# define filters
target_rev <- "DCRB"         # revenue target
target_lbs <- "DCRB"         # landings target
min_depth <- 0               # minimum depth in meters
max_depth <- -150            # maximum depth in meters
min_speed <- 0               # minimum speed in m/s
max_speed <- 4.11556         # maximum speed in m/s (4.11556 m/s = 8 knots)
crab_year_start <- 11        # month defines start of crab year
winter_months <- c("November", "December", "January", "February", "March") # determine Winter or Spring-Summer season
```

```{r transform-ticket-data}
# apply same filters as VMS dataframe
dcrb_ticket_df <- ticket_df %>%
  # add temporal columns
  mutate(
    year_numeric = year(date),
    month_numeric = month(date),
    week_numeric = week(date),
    day_numeric = yday(date),
    month_factor = month(date, label = TRUE, abbr = FALSE),
    year_month_character = paste0(year(date),"_", substr(ymd(date), 6, 7)),
    crab_year_character = ifelse(month_numeric >= crab_year_start, 
                                 paste0(year_numeric, "_", 1+year_numeric),
                                 paste0(year_numeric-1, "_", year_numeric)),
    season_character = as.character(ifelse(month_factor %in% winter_months, "Winter", "Spring-Summer")),
    year_month_date = ym(year_month_character)
  ) %>%
  # apply filters
  filter(TARGET_rev == target_rev | TARGET_lbs == target_lbs) %>%
  # join inflation adjustment factor and adjust revenue
  left_join(gdp_defl, by = join_by(year_numeric == year)) %>%
  mutate(DCRB_revenue_afi = DCRB_revenue / inflation_adjustment_factor) %>%
  # select columns
  dplyr::select(
    # identifiers
    Rec_ID,           # fish ticket ID
    drvid,            # vessel ID
    pacfin_port_code, # port ID
    port_group_code,  # port group ID
    agency_code,      # agency code
    # temporal fields
    date,
    year_month_date,
    crab_year_character,
    year_month_character,
    month_factor,
    year_numeric,
    month_numeric,
    week_numeric,
    day_numeric,
    # vessel length
    FINAL_LENGTH,
    # dungeness crab fields
    DCRB_lbs,
    DCRB_revenue_afi
  ) %>%
  # de-duplicate records
  distinct()

# take a peek at the resulting dataframe
glimpse(dcrb_ticket_df)
```

```{r}
# count total records, trips and years
n_records <- nrow(dcrb_ticket_df)
n_trips   <- n_distinct(dcrb_ticket_df$Rec_ID)
n_years   <- n_distinct(dcrb_ticket_df$year_numeric)
```

The Dungeness crab fish ticket dataframe has `r n_records` records (fish tickets), `r n_trips` distinct trips (fish tickets), across `r n_years` years.

There are more records than trips due to the vessel registration processing code including the lookback window data in step 2 of the pipeline. I looked at some examples of duplicates, and they occur for tickets within the lookback window, having a vessel final length in one year and not in another. That can be fixed in the plotting code by removing duplicates when one length is NA.

```{r}
# how many fish tickets have duplicate records?
n_records - n_trips # 7499

# explore why n_records != n_trips
qa_df <- dcrb_ticket_df %>%
  group_by(Rec_ID, year_numeric, agency_code) %>%
  summarize(n_records = n(),
            na_length = sum(is.na(FINAL_LENGTH)), .groups = 'drop')

# what's the max # duplicates?
max(qa_df$n_records) # 2 records

# are the duplicates all from NA length?
max(qa_df$na_length) # max 1 record has NA length per fish ticket
qa_df %>% filter(n_records > 1) %>% summarize(sum(na_length)) # yes, all 7499 records

# are there additional non-duplicates with NA length?
qa_df %>% summarize(sum(na_length)) # yes, 12909 records have NA length, which is greater than the 7499 which have an NA and non-NA length

# are the NAs from a particular agency or year?
qa_df %>% filter(n_records > 1) %>% group_by(agency_code) %>% count() %>% arrange(agency_code) # no, some in every agency
qa_df %>% filter(n_records > 1) %>% group_by(year_numeric) %>% count() %>% arrange(year_numeric) # no, some in every year
```

I'm seeing the duplication of some records, which seems to be due to the same fish ticket having an NA and a non-NA vessel length. I'm going to check data sources in this order for where the duplication of records is happening:

* vessel key in step 2 of main process script (checked, not here)
* fish ticket to vessel key join in step 2 of main process script (checked, not here)
* multiple years joined together in this script (suspect it's here)
* VMS data joined to fish ticket data after step 2 in main process script (not yet checked)
* fishing activity maps / metrics in this script (not yet checked)

I think this issue is new after the repository reorganization, because looking at my old VMS representativeness plots (from Jan.), there isn't a huge chunk of NA vessel lengths. There were some in 2023, 2021 and 2019, but not a huge amount.

```{r}
# load vessel length keys
vessel_df <- purrr::map(load_years, function(ly) {
  read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'vessel_length_keys', paste0('vessel_length_key_', ly, '.rds')))
}) %>% bind_rows()

# check how many records per vessel ID per year, and how many are NA length
qa_vessel_df <- vessel_df %>%
  group_by(drvid, year) %>% 
  summarize(count = n(), 
            na_length = sum(is.na(FINAL_LENGTH))) %>%
  arrange(desc(na_length), desc(count))

# where are the NAs from?
max(qa_vessel_df$count) # 3 records max for one year and one vessel
max(qa_vessel_df$na_length) # 1 NA length record max for one year and one vessel
sum(qa_vessel_df$count > 1 & qa_vessel_df$na_length < qa_vessel_df$count) # 3274 vessel IDs have multiple records, one of which is NA length

# try looking at one year, and see if the issue is there
vessel_df_2014 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'vessel_length_keys', paste0('vessel_length_key_2014.rds')))

# repeat QA dataframe
qa_vessel_2014_df <- vessel_df_2014 %>%
  group_by(drvid, year) %>% 
  summarize(count = n(), 
            na_length = sum(is.na(FINAL_LENGTH))) %>%
  arrange(desc(na_length), desc(count))

# repeat checks - where are the NAs from?
max(qa_vessel_2014_df$count) # 3 records max for one year and one vessel
max(qa_vessel_2014_df$na_length) # 1 NA length record max for one year and one vessel
sum(qa_vessel_2014_df$count > 1 & qa_vessel_df$na_length < qa_vessel_df$count) # 4357 vessel IDs have multiple records, one of which is NA length
qa_vessel_2014_df %>% group_by(count, na_length) %>% summarize(n = n()) %>% arrange(desc(n)) # most vessel IDs are fine, the more common issue than NA length is different lengths

# confirm only one year of data is in vessel length dataframes
unique(qa_vessel_2014_df$year) # confirmed, only 2014 assigned as year
```

The issue is in the vessel length keys. But I'm not really sure what's causing it. I took a quick look at the dataframe for vessels with multiple lengths, and found that including agency_code creates the issue. Since the last time I plotted was just for California, I didn't encounter this issue?

* Checking vessel IDs with 3 vessel lengths, 0 NA vessel length
  * One example had lengths of 40, 40, and 41 based for C, O, and W.
  * Another example had the same issue, 50, 44, 48 lengths for different agency codes
  * Another example had the same issue, 54, 53, 60 lengths for different agency codes
* Check vessel IDs with 2 vessel lengths, 1 NA vessel length
  * One example had NA for O, 50 for W
  * Another example had NA for O, 31 for W
  * Another example had NA for O, 28 for W

So it's fine that multiple vessels have different registrations and therefore different lengths across agencies. It does mean that for representativeness plots, they should be split by agency or de-duplicated across states, otherwise vessels will be double- or triple-counted if they have registrations and tickets in multiple states.

But why would there be multiple RecIDs for the same year in ticket_df? I thought fish tickets were unique to a given state. Did I remove something differentiating in the dcrb_ticket_df filter?

```{r}
# explore why n_records != n_trips
qa_ticket_df <- ticket_df %>%
  group_by(Rec_ID) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
```

No, still seeing duplicates with the same length issue in un-filtered version.

```{r}
# try looking at one year before vessel registration join, and see if the issue is there
ticket_df_2014 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'fish_tickets', 'fishtix_withFTID_2014.rds'))

ticket_df %>%
  filter(year == 2014) %>%
  group_by(Rec_ID, agency_code) %>%
  summarize(n_records = n(),
            na_length = sum(is.na(FINAL_LENGTH)), .groups = 'drop') %>% arrange(desc(n_records))

ticket_df_2014 %>%
  group_by(Rec_ID, agency_code) %>%
  summarize(n_records = n()) %>% arrange(desc(n_records))
```

```{r}
# try looking at one year after vessel registration join, and see if the issue is there
ticket_vessel_df_2014 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'fish_tickets', 'fishtix_vlengths_withFTID_2014.rds'))
ticket_vessel_df_2013 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'fish_tickets', 'fishtix_vlengths_withFTID_2013.rds'))
ticket_vessel_df_2015 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'fish_tickets', 'fishtix_vlengths_withFTID_2015.rds'))

ticket_vessel_df_2014 %>%
  group_by(Rec_ID, agency_code) %>%
  summarize(n_records = n(),
            na_length = sum(is.na(FINAL_LENGTH)), .groups = 'drop') %>% arrange(desc(n_records))

ticket_df_2014 %>%
  group_by(Rec_ID, agency_code) %>%
  summarize(n_records = n()) %>% arrange(desc(n_records))

```

I took another look at the problematic records, and they occur in the lookback window timeframe (Dec. 26-31 of the prior year). I'm wondering how to fix that.

```{r}
vms_2014 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'filtered', 'matched_filtered_withFTID_length_2014.rds'))
vms_2015 <- read_rds(here('Confidential', 'processed_data', 'processed_2025-03-19', 'filtered', 'matched_filtered_withFTID_length_2015.rds'))

# check 2014 VMS count
qa_vms_2014_df <- vms_2014 %>%
  group_by(Rec_ID, agency_code, drvid) %>%
  summarize(n = n(),
            na_length = sum(is.na(FINAL_LENGTH))) %>%
  arrange(desc(na_length))

# check out results
glimpse(qa_vms_2014_df)
```

Seems like the issue isn't happening at VMS data then. Perhaps the issue is just related to joining data in this step for the time series visualizations.


