---
title: "Overlay VMS points grid of choice and calcailate value of each 5km x 5km grid cell per year for dungeness crab"
output: 
  html_document:
    toc: true
    toc_float: true
---

### Set up

Attribute the VMS and fish ticket pipeline output for dungeness crab to each grid cell, then summarize for two baseline periods and one implementation period. Created for CDFW data request by Oct. 1, 2024.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(sf)
library(here)
library(knitr)
library(lubridate)
library(glue)
library(readr)
library(rnaturalearth) # for mapping states
library(viridis) # for good color scales
```

### Import data

Load VMS pings with fish tickets for all years. Output is from Blake, run for dungeness crab on Sept. 17, 2024.

```{r import all}
# clear workspace of everything but 5km grid
rm(list=setdiff(ls(),'grd'))

# load 2014-2023 fish ticket and VMS data
vms_all <- read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2014_matched_filtered_withFTID_length.rds')) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2015_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2016_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2017_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2018_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2019_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2020_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2021_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2022_matched_filtered_withFTID_length.rds'))) %>%
  bind_rows(read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2023_matched_filtered_withFTID_length.rds')))

# take a look at imported features
# can change back to glimpse for example records
names(vms_all)
# check all years are present
unique(year(vms_all$date))
```

### Match VMS pings and fish tickets to grid cells

Load 5km grid. I suspect there's some redundancy here in loading and joining the grid, but I've left it as is for now.

```{r}

# load 5x5 grid shapefile
grd <- read_sf(here('GIS_layers', 'master_5km_grid_tmer.shp'))

# convert VMS to spatial object
pt <- proc.time()
vms_all_sf <- vms_all %>%
  st_as_sf(coords= c('LON','LAT'), crs=4326) %>% 
  # then, convert to planar projection to match the grid
  st_transform(st_crs(grd))
x <- proc.time() - pt
```

Took `r round(x[3]/60,2)` minutes to do the conversion. Now for the join...

```{r join VMS with spatial object to grid}
# do the join
pt <- proc.time()
vms_all_grd_match <- vms_all_sf %>%
  st_join(grd)

# explore output
names(vms_all_grd_match)

# check timing
x <- proc.time() - pt
```

The join took `r round(x[3]/60,2)` minutes.

### Write joined data

```{r}
# save without geometry while still including grid cell ID
vms_all_grd_match %>% 
  st_set_geometry(NULL) %>% 
  write_rds(here('Confidential', 'data', 'interim', 'vms_all_w_grd_5km.rds'))

# load version without geometry with grid cell ID written above
vms_all_grd_match <- read_rds(here('Confidential', 'data', 'interim', 'vms_all_w_grd_5km.rds'))
names(vms_all_grd_match)
```

### Filter data and add date columns

Now filter the data based on control variables below: dungeness crab target, landed in California, depth <= 150m, and vessel speeds <= 4 knots.
Then add date columns and select subset of columns.

Notes:

* *crab_year* is Nov.-Oct. (e.g. Nov. 2019 to Oct. 2020 for 2019_2020), rather than the precise dates of the crab fishing season.
* *agency_code* filters for VMS pings joined to fish tickets that were landed in California. You can see in the map at the end of the file this means some pings extend beyond California.
* *removal_type* to filter for commercial landings is not included here, so personal take is included in *DCRB_lbs*. This does not impact *DCRB_VMS_pings* or *DCRB_rev*.

```{r filter fishing data}

# set filter parameters
state_agency_code = "C" # filter for landings brought to California ports
target_rev <- "DCRB" # I suspect this filter is redundant since Blake ran the pipeline for only DCRB, but I left it for now
target_lbs <- "DCRB" # again, I suspect this filter is redundant, but left it for now
winter_months <- c("November", "December", "January", "February", "March") # used to set season as Winter or Spring-Summer
min_depth <- 0
max_depth <- -150  # filter for 0-150 M depth
max_speed <- 4.11556 # units of m/s. 4.11556m/s = 8knots (4DCRB) and 1.54333 m/s = 3 knots (4CHNK)
min_speed <- 0 # units = m/s

# set fishing seasons to include
crab_years_to_vis = c("2014_2015", "2015_2016", "2016_2017", "2017_2018", "2018_2019",
                      "2019_2020", "2020_2021", "2021_2022", "2022_2023")

# filter data, add year/season columns, and select columns for analysis
dcrb_vms_tix_analysis <- vms_all_grd_match %>%
  filter(agency_code == state_agency_code) %>%
  filter(TARGET_rev == target_rev | TARGET_lbs == target_lbs) %>%
  filter(NGDC_M <= min_depth & NGDC_M >= max_depth) %>%
  filter(avg_speed_recalc <= max_speed & avg_speed_recalc >= min_speed) %>%
  mutate(
    year = lubridate::year(westcoastdate_notime),
    year_month = paste0(lubridate::year(westcoastdate_notime),"_", substr(lubridate::ymd(westcoastdate_notime),6,7)),
    # substr() ensures month is a 2 digit value, e.g. February is "02" not "2"
    month = lubridate::month(westcoastdate_notime, label=TRUE, abbr = FALSE),
    month_as_numeric = month(westcoastdate_notime),
    season = as.character(ifelse(month %in% winter_months, "Winter", "Spring-Summer")),
    crab_year = ifelse(
      month_as_numeric >= 11, paste0(year, "_", 1+year), paste0(year-1, "_", year)
    )
  ) %>%
  filter(crab_year %in% crab_years_to_vis) %>%
  dplyr::select(
      GRID5KM_ID, # grid cell ID
      Rec_ID, # fish ticket ID
      VMS_RECNO, # VMS ping ID
      drvid, # vessel ID
      westcoastdate_notime,
      year,
      crab_year, 
      year_month,
      month, 
      month_as_numeric,
      season,
      pacfin_port_code, 
      port_group_code, 
      NGDC_M, # depth
      TARGET_lbs, # target species by weight (filtered to "DCRB")
      TARGET_rev, # target species by revenue (filtered to "DCRB")
      DCRB_lbs, # dungeness crab catch weight
      DCRB_revenue # dungeness crab catch revenue
    )

# take a look at transformed data
names(dcrb_vms_tix_analysis)

# check all crab years present
unique(dcrb_vms_tix_analysis$crab_year)
```

### Distribute $ and lbs to grid cells and normalize pings

Distribute rev and lbs to grid cells. Normalize VMS pings by year-month and grid cell.

```{r distribute $ and lbs to cells}

# create new columns that attribute pings, lbs, and $ from each fish ticket 
# based on proportion of pings related to each ticket in each grid cell

# total records per trip (total number of VMS records associated with each fish ticket)
# one record = one fish ticket
VMSrecords_per_trip <- dcrb_vms_tix_analysis %>%
  group_by(Rec_ID) %>%
  summarise(trip_VMSrecords = n(),
            .groups = 'drop')

# join total records per trip to VMS and fish ticket data and calculate vessels, lbs, and $ per VMS ping
dcrb_vms_tix_analysis_TripInfo <- left_join(VMSrecords_per_trip, dcrb_vms_tix_analysis, by="Rec_ID") %>%
  mutate(
    DCRB_lbs_per_VMSlocation = DCRB_lbs/trip_VMSrecords,
    DCRB_rev_per_VMSlocation = DCRB_revenue/trip_VMSrecords,
    DCRB_Vessels_per_VMSlocation = 1/trip_VMSrecords
  )
names(dcrb_vms_tix_analysis_TripInfo)

# summarize and normalize fishery effort at year, month and grid cell level
# based on Jameal's prep_data_for_scenario_df_function.R
dcrb_year_month_5km_df <- dcrb_vms_tix_analysis_TripInfo %>%
  group_by(crab_year, year, year_month, month_as_numeric, month, GRID5KM_ID) %>%
  summarise(
    DCRB_lbs = sum(DCRB_lbs_per_VMSlocation),
    DCRB_rev = sum(DCRB_rev_per_VMSlocation),
    DCRB_VMS_pings = n(),
    DCRB_Vessels = sum(DCRB_Vessels_per_VMSlocation),
    Unique_DCRB_Vessels = length(unique(as.character(drvid)))
  ) %>%
  ungroup() %>%
  mutate(
    # rescale pings to 0-1
    normalized_DCRB_VMS_pings = as.vector(scale(DCRB_VMS_pings, center = min(DCRB_VMS_pings), scale = diff(range(DCRB_VMS_pings)))),
    # above rescaling converts 1 ping to 0, so change it so that 1 ping, after re-scaling, has a scaled value equal to one half of the rescaled value of 2 pings
    normalized_DCRB_VMS_pings = ifelse(
      normalized_DCRB_VMS_pings == 0, 
      0.5 * min(normalized_DCRB_VMS_pings[normalized_DCRB_VMS_pings != min(normalized_DCRB_VMS_pings)]),
      normalized_DCRB_VMS_pings
    )
  )

# save interim outputs
write_rds(dcrb_vms_tix_analysis_TripInfo, here::here('Confidential', 'data', 'interim', 'dcrb_vms_tix_analysis_TripInfo.rds'))
write_rds(dcrb_year_month_5km_df, here::here('Confidential', 'data', 'interim', 'dcrb_year_month_5km_df.rds'))

# visualize # vms pings
dcrb_year_month_5km_df %>%
  ggplot(aes(DCRB_VMS_pings)) + geom_histogram(binwidth=5)
print(range(dcrb_year_month_5km_df$DCRB_VMS_pings))

# visualize normalized # vms pings
dcrb_year_month_5km_df %>%
  ggplot(aes(normalized_DCRB_VMS_pings)) + geom_histogram(binwidth=0.01)
print(range(dcrb_year_month_5km_df$normalized_DCRB_VMS_pings))
```

### Create summaries for Heather

* *Baseline 2014-2019* includes the start of Nov. 2014 to the end of Oct. 2019.
* *Baseline 2017-2019* includes the start of Nov. 2017 to the end of Oct. 2019.
* *Implementation 2019-2023* includes the start of Nov. 2019 to the end of Oct. 2023.

For baseline periods, each record represents one grid cell for one month, across all years. Fishing effort measured by # of pings is averaged across years for a given month. Note that the # of unique vessels is the minimum for that grid cell and month in any year, let me know if I should update this.

For implementation period, each record represents one grid cell for one month in one year. 

```{r create summaries}
# set baseline and implementation years
baseline_2014_2019_crab_years <- c("2014_2015", "2015_2016", "2016_2017", "2017_2018", "2018_2019")
baseline_2017_2019_crab_years <- c("2017_2018", "2018_2019")
implementation_2019_2023_crab_years <- c("2019_2020", "2020_2021", "2021_2022", "2022_2023")

# summarize baseline data (2014-2018) at monthly temporal grain and grid cell ID spatial grain
baseline_2014_2019 <- dcrb_year_month_5km_df %>%
  filter(crab_year %in% baseline_2014_2019_crab_years) %>%
  group_by(GRID5KM_ID, month, month_as_numeric) %>%
  summarise(
    # take average by summing across years and dividing by # years
    # use sum / length instead of mean, because grid cells with 0 pings have no records, rather than a record with '0'
    avg_DCRB_lbs = sum(DCRB_lbs) / length(baseline_2014_2019_crab_years),
    avg_DCRB_rev = sum(DCRB_rev) / length(baseline_2014_2019_crab_years),
    avg_DCRB_VMS_pings = sum(DCRB_VMS_pings) / length(baseline_2014_2019_crab_years),
    avg_normalized_DCRB_VMS_pings = sum(normalized_DCRB_VMS_pings) / length(baseline_2014_2019_crab_years),
    avg_DCRB_Vessels = sum(DCRB_Vessels) / length(baseline_2014_2019_crab_years),
    # take minimum unique DCRB vessels across years
    min_Unique_DCRB_Vessels = min(Unique_DCRB_Vessels),
    .groups = 'drop'
  )
# explore output
names(baseline_2014_2019)
summary(baseline_2014_2019$avg_DCRB_VMS_pings)
summary(baseline_2014_2019$avg_normalized_DCRB_VMS_pings)
sort(unique(baseline_2014_2019$month_as_numeric))
# write output
write_rds(baseline_2014_2019, here::here('Confidential', 'processed', 'summaries', 'baseline_2014_2019.rds'))

# summarize baseline data (2017-2018) at monthly temporal grain and grid cell ID spatial grain
baseline_2017_2019 <- dcrb_year_month_5km_df %>%
  filter(crab_year %in% baseline_2017_2019_crab_years) %>%
  group_by(GRID5KM_ID, month, month_as_numeric) %>%
  summarise(
    # take average by summing across years and dividing by # years
    # use sum / length instead of mean, because grid cells with 0 pings have no records, rather than a record with '0'
    avg_DCRB_lbs = sum(DCRB_lbs) / length(baseline_2017_2019_crab_years),
    avg_DCRB_rev = sum(DCRB_rev) / length(baseline_2017_2019_crab_years),
    avg_DCRB_VMS_pings = sum(DCRB_VMS_pings) / length(baseline_2017_2019_crab_years),
    avg_normalized_DCRB_VMS_pings = sum(normalized_DCRB_VMS_pings) / length(baseline_2017_2019_crab_years),
    avg_DCRB_Vessels = sum(DCRB_Vessels) / length(baseline_2017_2019_crab_years),
    # take minimum unique DCRB vessels across years
    min_Unique_DCRB_Vessels = min(Unique_DCRB_Vessels),
    .groups = 'drop'
  )
# explore output
names(baseline_2017_2019)
sort(unique(baseline_2017_2019$month_as_numeric))
summary(baseline_2017_2019$avg_DCRB_VMS_pings)
summary(baseline_2017_2019$avg_normalized_DCRB_VMS_pings)
# write output
write_rds(baseline_2017_2019, here::here('Confidential', 'processed', 'summaries', 'baseline_2017_2019.rds'))

# write real-time data (2019-2023) at yearly and monthly temporal grain and grid cell ID spatial grain
implementation_2019_2023 <- dcrb_year_month_5km_df %>%
  filter(crab_year %in% implementation_2019_2023_crab_years)
# explore output
names(implementation_2019_2023)
sort(unique(implementation_2019_2023$month_as_numeric))
summary(implementation_2019_2023$DCRB_VMS_pings)
summary(implementation_2019_2023$normalized_DCRB_VMS_pings)
# write output
write_rds(implementation_2019_2023, here::here('Confidential', 'processed', 'summaries', 'implementation_2019_2023.rds'))
```

### Map pings

For a common sense check, see how the baseline periods and implementation period fishing effort look on a map. Visualize average # of pings for baseline periods, and # of pings for implementation period.

```{r visualize on map}

# based on Owen's compare_hw_bh.R

# West coast, using the rnaturalearth package (for a background map)
coaststates <- ne_states(country='United States of America',returnclass = 'sf') %>% 
  filter(name %in% c('California','Oregon','Washington','Nevada')) %>% 
  # spatially tranform to the same coordinate ref system as the 5k grid
  st_transform(st_crs(grd))

# visualize all years
# join summary back to geometry by GRID5KM_ID
dcrb_year_month_5km_df_with_grid <- dcrb_year_month_5km_df %>% 
  left_join(grd, by = join_by(GRID5KM_ID)) %>%
  st_as_sf()
# set bounding box 
bbox <- st_bbox(dcrb_year_month_5km_df_with_grid)
# plot log of avg. pings on a map
map_log_pings_dcrb_year_month_5km_df <- ggplot() +
  # background map
  geom_sf(data = coaststates, fill = 'gray80') +
  # 5KM grid with pings. Scale fill color by number of pings
  geom_sf(data = dcrb_year_month_5km_df_with_grid, aes(fill = log10(DCRB_VMS_pings)), color = NA) +
  facet_grid(. ~ year) + # just added this TODO check it
  scale_fill_viridis() +
  # set manual bounding box
  xlim(bbox[1], bbox[3]) + ylim(bbox[2], bbox[4]) +
  labs(fill="log10(Pings) per 5x5 cell") +
  theme_minimal()
map_log_pings_dcrb_year_month_5km_df

# visualize 2014-2018 baseline
# join summary back to geometry by GRID5KM_ID
baseline_2014_2019_with_grid <- baseline_2014_2019 %>% 
  left_join(grd, by = join_by(GRID5KM_ID)) %>%
  st_as_sf()
# set bounding box 
bbox <- st_bbox(baseline_2014_2019_with_grid)
# plot log of avg. pings on a map
map_log_pings_baseline_2014_2019 <- ggplot() +
  # background map
  geom_sf(data = coaststates, fill = 'gray80') +
  # 5KM grid with pings. Scale fill color by number of pings
  geom_sf(data = baseline_2014_2019_with_grid, aes(fill = log10(avg_DCRB_VMS_pings)), color = NA) +
  scale_fill_viridis() +
  # set manual bounding box
  xlim(bbox[1], bbox[3]) + ylim(bbox[2], bbox[4]) +
  labs(fill="log10(Avg. Pings) per 5x5 cell") +
  theme_minimal()
map_log_pings_baseline_2014_2019

# visualize 2017-2018 baseline
# join summary back to geometry by GRID5KM_ID
baseline_2017_2019_with_grid <- baseline_2017_2019 %>% 
  left_join(grd, by = join_by(GRID5KM_ID)) %>%
  st_as_sf()
# set bounding box
bbox <- st_bbox(baseline_2017_2019_with_grid)
# plot log of avg. pings on a map
map_log_pings_baseline_2017_2019 <- ggplot() +
  # background map
  geom_sf(data = coaststates, fill = 'gray80') +
  # 5KM grid with pings. Scale fill color by number of pings
  geom_sf(data = baseline_2017_2019_with_grid, aes(fill = log10(avg_DCRB_VMS_pings)), color = NA) +
  scale_fill_viridis() +
  # set manual bounding box
  xlim(bbox[1], bbox[3]) + ylim(bbox[2], bbox[4]) +
  labs(fill="log10(Avg. Pings) per 5x5 cell") +
  theme_minimal()
map_log_pings_baseline_2017_2019

# visualize 2019-2023 implementation period
# join summary back to geometry by GRID5KM_ID
implementation_2019_2023_with_grid <- implementation_2019_2023 %>% 
  left_join(grd, by = join_by(GRID5KM_ID)) %>%
  st_as_sf()
# set bounding box
bbox <- st_bbox(implementation_2019_2023_with_grid)
# plot log of avg. pings on a map
map_log_pings_implementation_2019_2023 <- ggplot() +
  # background map
  geom_sf(data = coaststates, fill = 'gray80') +
  # 5KM grid with pings. Scale fill color by number of pings
  geom_sf(data = implementation_2019_2023_with_grid, aes(fill = log10(DCRB_VMS_pings)), color = NA) +
  facet_grid(. ~ crab_year) +
  scale_fill_viridis() +
  # set manual bounding box
  xlim(bbox[1], bbox[3]) + ylim(bbox[2], bbox[4]) +
  labs(fill="log10(Pings) per 5x5 cell") +
  theme_minimal()
map_log_pings_implementation_2019_2023
```

### Explore impact of filters on Nov. and Dec. 2023 data

I've left these filter summaries here in case they're useful in the future, but set the code block to not evaluate.

I'd written them to figure out what # and % of data was dropped by each filter, when I thought there was an issue with 2023-2024 data (there was not).

```{r investigate 2023 lack of data, eval = FALSE}
# load 2023 data
vms_2023 <- read_rds(here('Confidential', 'processed', 'matched', 'filtering', '2023_matched_filtered_withFTID_length.rds'))

# load and join spatial data
# I still don't fully understand each step here bouncing between sf and tbl, but that's fine for now
vms_2023_sf <- vms_2023 %>%
  st_as_sf(coords= c('LON','LAT'), crs=4326) %>% 
  # then, convert to planar projection to match the grid
  st_transform(st_crs(grd))
vms_2023_all_grd_match <- vms_2023_sf %>% 
  st_join(grd) %>%
  st_set_geometry(NULL)

# set filter parameters
state_agency_code = "C" # filter for landings brought to California ports
target_rev <- "DCRB"
target_lbs <- "DCRB"
winter_months <- c("November", "December", "January", "February", "March") # used to set season as Winter or Spring-Summer
min_depth <- 0
max_depth <- -150  # filter for 0-150 M depth
max_speed <- 4.11556 # units of m/s. 4.11556m/s = 8knots (4DCRB) and 1.54333 m/s = 3 knots (4CHNK)
min_speed <- 0 # units = m/s

# look into which filter leads to biggest drop
dcrb_vms_tix_analysis_2023 <- vms_2023_all_grd_match %>%
  mutate(
    # if a record would be filtered out, then filter_ columns have value of 1
    # if a record would not be filtered out, then filter_ columns have value of 0
    filter_agency = case_when(agency_code == state_agency_code ~ 0, .default = 1),
    filter_target = case_when((TARGET_rev == target_rev | TARGET_lbs == target_lbs) ~ 0, .default = 1),
    filter_speed = case_when((avg_speed_recalc <= max_speed & avg_speed_recalc >= min_speed) ~ 0, .default = 1),
    filter_depth = case_when(NGDC_M <= min_depth & NGDC_M >= max_depth ~ 0, .default = 1),
    filter_any = case_when(filter_agency + filter_target + filter_depth + filter_speed > 0 ~ 1, .default = 0),
    year = lubridate::year(westcoastdate_notime),
    year_month = paste0(lubridate::year(westcoastdate_notime),"_", substr(lubridate::ymd(westcoastdate_notime),6,7)),
    # substr() ensures month is a 2 digit value, e.g. February is "02" not "2"
    month = lubridate::month(westcoastdate_notime, label=TRUE, abbr = FALSE),
    month_as_numeric = month(westcoastdate_notime),
    season = as.character(ifelse(month %in% winter_months, "Winter", "Spring-Summer")),
    crab_year = ifelse(
      month_as_numeric >= 11, paste0(year, "_", 1+year), paste0(year-1, "_", year)
    )
  )

# check # records vs. crab_year and month after transformation
dcrb_vms_tix_analysis_2023 %>% 
  group_by(month_as_numeric) %>%
  summarize(
    n_records = n(),
    n_filter_agency = sum(filter_agency),
    n_filter_target = sum(filter_target),
    n_filter_speed = sum(filter_speed),
    n_filter_depth = sum(filter_depth),
    n_filter_any = sum(filter_any)
  )

# check % records vs. crab_year and month after transformation
dcrb_vms_tix_analysis_2023 %>% 
  group_by(month_as_numeric) %>%
  summarize(
    percent_filter_agency = sum(filter_agency) / n() * 100,
    percent_filter_target = sum(filter_target) / n() * 100,
    percent_filter_speed = sum(filter_speed) / n() * 100,
    percent_filter_depth = sum(filter_depth) / n() * 100,
    percent_filter_any = sum(filter_any) / n() * 100
  )

# after conversation with Blake and Owen, seems reasonable there's hardly any data for Nov./Dec. 2023
# excluding 2023_2024 from this analysis, since 2024 data not ready
```

### Visualize ping rate

At some point, the VMS ping rate increased from ~1/hour to ~1/15 minutes. As a result, in the time frame with the increased ping rate, fishing effort will look ~4x higher (when it's really just a change in sampling rate), and DCRB_lbs and DCRB_rev are spread over a higher resolution track (4x as many pings to attribute a single fish ticket to).

I'm visualizing this in two ways. 

1. Visualize time series of avg. pings per ticket tickets over time. I'm looking for an increase at a particular point in time, after which I will apply a correction for the increased ping rate. I also visualized number of tickets, just to explore for my own learning.
2. Visualize distribution of total and normalized pings per grid cell over time. Again, I'm lookign for an increase at a particular point in time.

```{r check when ping rate increased}

# approach 1: look at avg. pings per ticket over time

# visualize average pings per ticket over time
avg_pings_per_ticket_over_time <- dcrb_vms_tix_analysis %>%
  group_by(westcoastdate_notime, Rec_ID) %>%
  summarise(trip_VMSrecords = n()) %>%
  ungroup() %>%
  group_by(westcoastdate_notime) %>%
  summarise(n_tickets = n(),
            avg_pings_per_ticket = mean(trip_VMSrecords))
# plot # pings and avg. pings per ticket vs. time, each point is one day
avg_pings_per_ticket_over_time %>% 
  pivot_longer(-westcoastdate_notime) %>%
  ggplot(aes(x = westcoastdate_notime, y = value)) +
    geom_point() +
    facet_grid(name ~ .)

# looks like the increase was in 2021, now find out which month...

# look at average pings per ticket over time in table
avg_pings_per_ticket_over_time <- dcrb_vms_tix_analysis %>%
  filter(year == 2021) %>%
  group_by(month, Rec_ID) %>%
  summarise(trip_VMSrecords = n()) %>%
  ungroup() %>%
  group_by(month) %>%
  summarise(n_tickets = n(),
            avg_pings_per_ticket = mean(trip_VMSrecords))
# plot # pings and avg. pings per ticket vs. time, each point is one month
avg_pings_per_ticket_over_time %>% 
  pivot_longer(-month) %>%
  ggplot(aes(x = month, y = value, color = name)) +
    geom_point() + 
    facet_grid(name ~ .)

# looks like the increase was in January, look for a specific day...

# look at average pings per ticket over time in table
avg_pings_per_ticket_over_time <- dcrb_vms_tix_analysis %>%
  filter(year == 2021 & month == 'January') %>%
  group_by(westcoastdate_notime, Rec_ID) %>%
  summarise(trip_VMSrecords = n()) %>%
  ungroup() %>%
  group_by(westcoastdate_notime) %>%
  summarise(n_tickets = n(),
            avg_pings_per_ticket = mean(trip_VMSrecords))
# plot # pings and avg. pings per ticket vs. time, each point is one day in Jan. 2021
avg_pings_per_ticket_over_time %>% 
  pivot_longer(-westcoastdate_notime) %>%
  ggplot(aes(x = westcoastdate_notime, y = value, color = name)) +
    geom_point() + geom_line() +
    facet_grid(name ~ .)

# looks like the increase was at the start of January 2021.

# approach 2: look at # pings and normalized # pings per grid cell over time
# 1 record = sum of pings per grid cell per year-month

# visualize distribution of pings in violin plot
dcrb_year_month_5km_df %>%
  ggplot(aes(x = factor(year), y = DCRB_VMS_pings)) + 
  geom_boxplot()

# see summary of pings by year
dcrb_year_month_5km_df %>%
  group_by(year) %>%
  summarise(
    min_DCRB_VMS_pings = min(DCRB_VMS_pings),
    median_DCRB_VMS_pings = median(DCRB_VMS_pings),
    mean_DCRB_VMS_pings = mean(DCRB_VMS_pings),
    max_DCRB_VMS_pings = max(DCRB_VMS_pings)
  )

# see summary of normalized pings by year
dcrb_year_month_5km_df %>%
  group_by(year) %>%
  summarise(
    min_normalized_DCRB_VMS_pings = min(normalized_DCRB_VMS_pings),
    median_normalized_DCRB_VMS_pings = median(normalized_DCRB_VMS_pings),
    mean_normalized_DCRB_VMS_pings = mean(normalized_DCRB_VMS_pings),
    max_normalized_DCRB_VMS_pings = max(normalized_DCRB_VMS_pings)
  )
```

Jameal shared that the ping rate increased 4x in April 2020 for 900/1200 vessels.

Look for a monthly factor to correct vessel ping rate. This is a rough/fast fix, and it is worth checking how sensitive cooccurrence metrics are based on this correction factor.

```{r quick plot - fish tickets and season length over years}
# is fishing effort (as # fish tickets / # days fishery open) consistent in the post-April 2020 period?

crab_years_to_vis = c("2014_2015", "2015_2016", "2016_2017", "2017_2018", "2018_2019",
                      "2019_2020", "2020_2021", "2021_2022", "2022_2023")

# how many days was the fishery open each season? how many fish tickets for each season?
# -"open" means there were VMS pings with fish tickets matched to them
# - crab_year represents a season and is defined earlier in this script as Nov. - Oct., it is not based on closures

# season length by year
n_fish_tickets_per_day_by_crab_year <- dcrb_vms_tix_analysis %>%
  filter(crab_year %in% crab_years_to_vis) %>%
  group_by(crab_year) %>%
  summarise(season_start_day = min(westcoastdate_notime),
            season_end_day = max(westcoastdate_notime),
            season_length = season_end_day - season_start_day,
            season_length_numeric = as.numeric(season_length, unit = 'days'),
            n_fish_tickets = n_distinct(Rec_ID),
            n_fish_tickets_per_season_day = n_fish_tickets / season_length_numeric)
# see table
n_fish_tickets_per_day_by_crab_year

# plot separately over time
plot_n_fish_tickets <- ggplot(n_fish_tickets_per_day_by_crab_year, aes(x = crab_year, y = n_fish_tickets, group = 1)) +
    geom_point() + geom_line() + ylim(0, max(n_fish_tickets_per_day_by_crab_year$n_fish_tickets))
plot_season_length <- ggplot(n_fish_tickets_per_day_by_crab_year, aes(x = crab_year, y = season_length, group = 1)) +
    geom_point() + geom_line() + ylim(0, max(n_fish_tickets_per_day_by_crab_year$season_length))
plot_n_fish_tickets_per_season_day <- ggplot(n_fish_tickets_per_day_by_crab_year, aes(x = crab_year, y = n_fish_tickets_per_season_day, group = 1)) +
    geom_point() + geom_line() + ylim(0, max(n_fish_tickets_per_day_by_crab_year$n_fish_tickets_per_season_day))
# give up and look at them individually...
plot_n_fish_tickets
plot_season_length
plot_n_fish_tickets_per_season_day


# TODO need to add month next
# if so, then look for month-wise correction in post-April 2020 period to apply to pings
```

```{r quick plot - # vessels before and after ping rate increase (Apr. 2020)}

monthly_df <- dcrb_vms_tix_analysis_TripInfo %>% 
  filter(year >= 2018 & year <= 2022) %>%
  mutate(month_as_date = round_date(westcoastdate_notime, unit='month')) %>%
  group_by(year, month_as_date) %>%
  summarise(n_vessels = n_distinct(drvid),
            n_tickets = n_distinct(Rec_ID),
            n_pings = n())
  
monthly_df

monthly_df %>% 
  ggplot(aes(x=month_as_date, y=n_vessels, color = as.factor(year))) + geom_point() + geom_line(group=1) +
    theme(axis.text.x = element_text(angle = 300))

monthly_df %>% 
  ggplot(aes(x=month_as_date, y=n_tickets, color = as.factor(year))) + geom_point() + geom_line(group=1) +
    theme(axis.text.x = element_text(angle = 300))

monthly_df %>% 
  ggplot(aes(x=month_as_date, y=n_pings, color = as.factor(year))) + geom_point() + geom_line(group=1) +
    theme(axis.text.x = element_text(angle = 300))

```

```{r calculate monthly correction for before and after ping rate increase}
# set the start date for ping rate increase to April 1, 2020
ping_rate_increase_date <- ymd(20200401)

# calculate the monthly ping rate per unique vessel before and after April 1, 2020
pre_post_ping_rate_increase <- dcrb_vms_tix_analysis_TripInfo %>%
  mutate(ping_rate_period = if_else(westcoastdate_notime < ping_rate_increase_date,
                                    "pre_ping_rate_increase",
                                    "post_ping_rate_increase")) %>% 
  group_by(ping_rate_period, month) %>%
  summarise(pings_per_vessel = n() / n_distinct(drvid)) # (# of pings) / (# of unique vessels)
# I'm not sure if month comes from fish ticket or from VMS ping time
# for this use case, I don't want fish tickets split across months, so I want the month of the fish ticket or of the trip start
# that may mean the variable in group_by needs to change

# plot the rates
pre_post_ping_rate_increase %>%
  ggplot(aes(x=month, y=pings_per_vessel, fill = ping_rate_period)) + 
  geom_col(position='dodge') +
  theme(axis.text.x = element_text(angle = 300))

# calculate the correction factor by dividing post period / pre period
#ping_rate_correction_factor <- 
pre_post_ping_rate_increase %>% 
  pivot_wider(names_from = ping_rate_period, values_from = pings_per_vessel) %>%
  mutate(ping_rate_correction = post_ping_rate_increase / pre_ping_rate_increase) %>%
  arrange(month)

```

