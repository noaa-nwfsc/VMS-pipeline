---
title: "Hindcasts of Oregon and Washington Dungeness crab fishing logbook activity"
author: "Brooke Hawkins"
date: "`r Sys.Date()`"
output: html_document
knit: |
  (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = paste0(xfun::sans_ext(input), '-', Sys.Date(), '.html')
    )
  })
---

## Purpose

* Integrate interpolated VMS and fish ticket data to produce time series, rasters, and maps of fishing activity, landed weight, and revenue.
* Deliverable: Monthly maps of fishing activity; tables summarizing monthly fishing activity

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# import libraries
library(tidyverse)
library(readxl)
library(here)
library(sf)
library(raster)
library(fasterize)
library(magrittr)
library(gridExtra)
library(nngeo)
library(rnaturalearth)
library(viridis)

# adjust ggplot theme
theme_replace(axis.text.x=element_text(angle=45, vjust=1, hjust=1),
              axis.ticks.x=element_blank(),
              axis.ticks.y=element_blank())
```

Clean up excel sheet and save as dataframe.

This is based on https://github.com/jameals/raimbow/blob/master/wdfw/code/OR/OR_logbook_analysis_script1_original_functions.R

```{r load-data}

# load OR logbook
df <- read_excel(here('Confidential', 'raw', 'logbook', 'CLBR110 Crab Logbook_pulled010722.xlsx'), sheet = "CLBR110")

# load season dates
season_dates_df <- read_excel(here('Confidential', 'raw', 'logbook', 'CLBR110 Crab Logbook_pulled010722.xlsx'), 
                              sheet = "SeasonDates",
                              col_names = c('crab_season', 'earliest_season_opening_date', 'description', 'season_close_date'),
                              skip = 1)

# transform date to date field
df$DetailDate <- mdy(df$DetailDate)

# based on a check I did in Excel, it didn't seem like anything had signs flipped or lat/lon flipped.

# if longitude or latitude set or up points are listed as 0, then convert to NA
# keep a boolean for whether any or all coordinates related to a string were NA
df <- df %>% mutate(SetLatDec = ifelse(SetLatDec < 1,  NA, SetLatDec),
                    UpLatDec  = ifelse(UpLatDec  < 1,  NA, UpLatDec),
                    SetLonDec = ifelse(SetLonDec > -1, NA, SetLonDec),
                    UpLonDec  = ifelse(UpLonDec  > -1, NA, UpLonDec),
                    all_na_coordinate_flag = is.na(SetLatDec) & is.na(SetLonDec) & is.na(UpLatDec) & is.na(UpLonDec),
                    any_na_coordinate_flag = is.na(SetLatDec) | is.na(SetLonDec) | is.na(UpLatDec) | is.na(UpLonDec))

# how many rows dropped for any NA coordinate? (#, %)
sum(df$any_na_coordinate_flag)
sum(df$any_na_coordinate_flag) / nrow(df)
# how many rows dropped for any NA coordinate? (#, %)
sum(df$all_na_coordinate_flag)
sum(df$all_na_coordinate_flag) / nrow(df)

# load 5km grid
grid_5km <- read_sf(here('GIS_layers', 'master_5km_grid_tmer.shp'))
```

```{r explore-logbook-data}
# how many trips?
n_distinct(df$LogID)
# how many strings?
n_distinct(df$LogDetailID)
# which crab years?
sort(unique(df$CrabYear))
# null count for each field?
# https://stackoverflow.com/questions/24027605/determine-the-number-of-na-values-in-a-column
summarise_all(df, ~sum(is.na(.)))

# when are pots pulled?
df %>% 
  dplyr::select(LogDetailID, DetailDate) %>% 
  group_by(DetailDate) %>%
  summarise(number_strings = n_distinct(LogDetailID)) %>%
  ggplot(aes(x = DetailDate, y = number_strings)) +
  geom_line() +
  scale_x_date(breaks = "6 months", date_labels = "%b %Y")

# what's the distribution of soak time?
df %>%
  dplyr::select(LogDetailID, SoakTime) %>%
  group_by(SoakTime) %>%
  summarise(number_strings = n_distinct(LogDetailID)) %>%
  ggplot(aes(x = SoakTime, y = number_strings)) + 
  geom_col()
summary(df$SoakTime)

# what's the distribution of coordinates?
summary(df$SetLatDec)
summary(df$SetLonDec)
summary(df$UpLatDec)
summary(df$UpLonDec)

# what's the distribution of line lengths? Need to calculate line length to do this... need to take points to lines to do this...

# try plotting the set lon/lat
set_sf <- df %>% 
  filter(!is.na(SetLonDec) & !is.na(SetLatDec)) %>%
  st_as_sf(coords = c('SetLonDec', 'SetLatDec'), crs = 4326)
up_sf <- df %>% 
  filter(!is.na(UpLonDec) & !is.na(UpLatDec)) %>%
  st_as_sf(coords = c('UpLonDec', 'UpLatDec'), crs = 4326)

# load west coast states simple features object for background
west_coast_states <- rnaturalearth::ne_states(country='United States of America', returnclass='sf') %>% 
  filter(name %in% c('California','Oregon','Washington')) %>% 
  st_transform(st_crs(grid_5km))

oregon <- rnaturalearth::ne_states(country='United States of America', returnclass='sf') %>% 
  filter(name %in% c('Oregon')) %>% 
  st_transform(st_crs(grid_5km))

# create map
ggplot() + geom_sf(data = west_coast_states, fill='gray80') +
  geom_sf(data = set_sf, aes(fill = Depth))

# create map
ggplot() + geom_sf(data = west_coast_states, fill='gray80') +
  geom_sf(data = up_sf, aes(fill = Depth))
```

```{r}
# filter logbook data
filtered_df <- df %>%
  filter(!is.na(SetLonDec) & !is.na(SetLatDec) & !is.na(UpLonDec) & !is.na(UpLatDec)) %>%
  filter(SpatialFlag == "False")
    
start_time <- proc.time()
# pivot from set and up points being in a single row to being in separate rows, with point_type of set or up
string_df <- filtered_df %>% pivot_longer(c(SetLatDec, SetLonDec, UpLatDec, UpLonDec),
                             names_to = c("point_type", ".value"),
                             names_pattern = "(.*)(Lat|Lon)Dec",
                             values_to = ".value") %>%
  # now drop point_type
  dplyr::select(-point_type) %>%
  # convert from doubles to sf points
  st_as_sf(coords = c("Lon", "Lat"), crs = 4326) %>%
  # group by logbook string and other columns in dataset
  group_by(LogID, LogDetailID, CrabPermit, CrabYear, DEP, DocNum, PortCode, DetailDate, Depth, NumPots, SoakTime, EstLbs, AdjLbs, AdjValue, TicketNum) %>%
  # transform sf points from degrees to meters for length calculations # https://epsg.io/32610
  st_transform(32610) %>%
  # convert sf points to sf linestring
  summarise(do_union = FALSE) %>% 
  st_cast("LINESTRING")
# print time
end_time <- proc.time()
cat((end_time - start_time)/60, 'minutes to do spatial conversion') # took 1.5 minutes

rm(start_time, end_time)

# how many strings?
nrow(string_df)

# calculate and store string length in meters
string_df$string_length_meters <- as.vector(st_length(string_df))

# what is the distribution of string lengths?
string_df %>% ggplot(aes(string_length_meters/1000)) + geom_histogram()
summary(string_df$string_length_meters/1000) # summarize in kilometers

# how many 0 line length?
sum(string_df$string_length_meters == 0) # count
sum(string_df$string_length_meters == 0) / nrow(string_df) * 100 # percent

# what's the 95th, 99th, 99.9th percentile for line length in kilometers?
quantile(string_df$string_length_meters, c(0.95, 0.97, 0.98, 0.99, 0.999)) / 1000

# what is the distribution of number of pots?
string_df %>% ggplot(aes(NumPots)) + geom_histogram()
summary(string_df$NumPots) 

# how many NA pots?
sum(is.na(string_df$NumPots)) # count
sum(is.na(string_df$NumPots)) / nrow(string_df) * 100 # percent
# what's the 95th, 99th, 99.9th percentile for number pots? (pretend NA's are 0's)
string_df$NumPots %>% replace_na(replace = 0) %>%
  quantile(c(0.95, 0.97, 0.98, 0.99, 0.999))
# what's the 95th, 99th, 99.9th percentile for number pots? (removing NAs)
quantile(string_df$NumPots, c(0.95, 0.97, 0.98, 0.99, 0.999), na.rm = TRUE)
# how many records have fractional number of pots?
sum(string_df$NumPots != round(string_df$NumPots), na.rm = TRUE) # none, amazing

# what to do with 0's? with really long? looks like prior cutoff was 25 km max, >0 km min
# for now, I'm choosing to keep any strings that have nothing enetered for # pots, since I'm not sure if it's used for scaling or simply replaced

start_time <- proc.time()
# distribute traps along the line
trap_df <- string_df %>%
  mutate(NumPots = replace_na(NumPots, 0)) %>%
  ungroup() %>%
  mutate(trap_locations = pmap(.l = list(NumPots, geometry),
                               .f = function(pots, string) st_line_sample(string, n = pots))) %>%
  mutate(trap_coordinates = map(.x = trap_locations,
                                 .f = function(traps) as_tibble(st_coordinates(traps)))) %>%
  unnest(trap_coordinates, keep_empty = TRUE) %>% # now, unnest arguments argue. what happens to rows with empty locations?
  st_set_geometry(NULL) %>%
  dplyr::select(-trap_locations, -L1) # L1 is an index created when extracting coordaintes, not needed, value is always 1. no longer need trap_locations, since coordinates extracted from it.

# print time
end_time <- proc.time()
cat((end_time - start_time)/60, 'minutes to do spatial conversion')
rm(start_time, end_time)

# quick check, are there NumPots number of trap_coordinates?
(sum(string_df$NumPots, na.rm = TRUE) + sum(is.na(string_df$NumPots))) == nrow(trap_df)

# string_df has NA for blanks, and trap_df has NA for X and Y
```


```{r}
# load raster
bathymetry_raster <- raster(here('data', 'bathymetry', 'composite_bath.tif'))
# explore raster, note units are in decimeters, divide by 10 to get meters
bathymetry_raster@extent
cellStats(bathymetry_raster, 'mean')
cellStats(bathymetry_raster, 'countNA')
cellStats(bathymetry_raster, 'max')
cellStats(bathymetry_raster, 'min')

# TODO bump this line further up, right after unnest
trap_sf <- trap_df %>%
  filter(NumPots != 0) %>%
  st_as_sf(coords = c('X', 'Y'), crs = 32610) %>%
  st_transform(4326)

# add depth to trap_sf
start_time <- Sys.time()
point_depths <- raster::extract(bathymetry_raster, trap_sf)
end_time <- Sys.time()
cat((end_time - start_time), 'minutes to do spatial conversion')
rm(start_time, end_time)
trap_sf$trap_coords_depth_m <- point_depths / 10 # divide by 10 to convert from decimeters to meters

# left off at line 245
trap_sf <- trap_sf %>%
  filter(depth <= 0) %>% # pots must be underwater
  filter(depth > -200) %>% # maximum depth of 200 meters underwater
  filter(depth <- 5000) # but also keep ports / embayments # TODO ask Owen about this one

# TODO check drop rate

# TODO ask about grid area key, seems different from current grid 5km? I think the purpose is to have area, so you can divide traps by area to get density (pots / km2)
# TODO consider renmaing trap_sf between these steps... or combining steps...
start_time <- Sys.time()
test_trap_df <- trap_sf %>%
  st_transform(st_crs(grid_5km)) %>% # transform to same coordinate reference system as grid
  st_join(grid_5km) %>% # join 5km grid to traps simple features
  st_set_geometry(NULL)
end_time <- Sys.time()
cat((end_time - start_time), 'minutes to do spatial conversion')
rm(start_time, end_time)

# create gridded summary (TODO figure out... why is this wrong? I think some double counting, need permit data, not sure how confidentiatlity works)
# TODO rename from test_trap_sf
crab_year_start <- 11        # month defines start of crab year
winter_months <- c("November", "December", "January", "February", "March") # determine Winter or Spring-Summer season
test_dcrb_5km_summary_df <- test_trap_df %>%
  mutate(
    year = year(DetailDate),
    month = month(DetailDate, label = TRUE, abbr = FALSE),
    month_numeric = month(DetailDate),
    year_month = paste0(year(DetailDate),"_", substr(ymd(DetailDate), 6, 7)),
    crab_year = ifelse(month_numeric >= crab_year_start, paste0(year, "_", 1+year), paste0(year-1, "_", year)),
    season = as.character(ifelse(month %in% winter_months, "Winter", "Spring-Summer")),
    week_of_year = week(DetailDate),
    day_of_year = yday(DetailDate)
  ) %>% # TODO include intermediate step where I write the pre-summarized output (should be for strings and pots)
  group_by(
    # grid cell ID, makes this a gridded summary
    GRID5KM_ID,
    # temporal columns, makes this a monthly summary
    year,
    crab_year,
    year_month,
    season,
    month,
    month_numeric
  ) %>%
  summarise(
    n_pots = n(),
    n_strings = n_distinct(LogDetailID),
    n_unique_vessels = n_distinct(DocNum), # is this equivalent to drvid? if not, why not? should this go to fish tickets?
    # what was confidential_flag doing here again?
    .groups = "keep"
  ) %>% 
  ungroup()
# TODO this is super not done, so do not trust. number pots needs more thought, no line length, no confidentiatlity, also just ugly code
# check out m2 code in script 2 https://github.com/jameals/raimbow/blob/master/wdfw/code/OR/OR_logbook_analysis_script2_M1_M2.R
```

```{r}

# st_area() computes area, but any gotchas? specific CRS to use?

# background map (coastline)
coaststates <- ne_states(country = 'United States of America', returnclass = 'sf') %>% 
  filter(name %in% c('California','Oregon','Washington')) %>%  
  st_transform(st_crs(grd))



```


Notes

* It seems suspicious to me there are *exactly* 160,000 strings in the logbook data.

TODOs

* Organize explorations more clearly
* Check # and % dropped in more systematic way (at each step)
* Check bathymetry and grid files with Blake
* Revisit depth filter options
* Figure out how to best place pots... undercount possible if removing on depth after placing, incorrect count possible if using reported instead of permit tier
