---
title: "Hindcasts of Oregon and Washington Dungeness crab fishing logbook activity"
author: "Brooke Hawkins"
date: "`r Sys.Date()`"
output: html_document
knit: |
  (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = paste0(xfun::sans_ext(input), '-', Sys.Date(), '.html')
    )
  })
---

## Purpose

* Integrate interpolated VMS and fish ticket data to produce time series, rasters, and maps of fishing activity, landed weight, and revenue.
* Deliverable: Monthly maps of fishing activity; tables summarizing monthly fishing activity

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# import libraries
library(tidyverse)
library(readxl)
library(here)
library(sf)
library(raster)
library(fasterize)
library(magrittr)
library(gridExtra)
library(nngeo)
library(rnaturalearth)
library(viridis)

# adjust ggplot theme
theme_replace(axis.text.x=element_text(angle=45, vjust=1, hjust=1),
              axis.ticks.x=element_blank(),
              axis.ticks.y=element_blank())
```

Reference code:

* https://github.com/jameals/raimbow/blob/master/wdfw/code/OR/OR_logbook_analysis_script1_original_functions.R

## Load data

1. Load the logbook data, which contains set and up locations for strings of crab pots.
  a. Load logbook data, which will be used to map monthly fishing activity distribution.
  b. Load tab with season dates, which can be used to filter fishing activity, but is not yet.
2. Load the bathymetry raster, which will be used to join depth information at a pot level.
3. Load the spatial 5km shapefile, which will be used to map monthly fishing activity distribution.

```{r load-data}
# load OR logbook
logbook_df <- read_excel(here('Confidential', 'data', 'raw', 'logbook', 'CLBR110 Crab Logbook_pulled010722.xlsx'), sheet = "CLBR110")

# load season dates
season_df <- read_excel(
  here('Confidential', 'data', 'raw', 'logbook', 'CLBR110 Crab Logbook_pulled010722.xlsx'), 
  sheet = "SeasonDates",
  col_names = c('crab_season', 'earliest_season_opening_date', 'description', 'season_close_date'),
  skip = 1
)

# load bathymetry raster
bathymetry <- raster(here('data', 'bathymetry', 'composite_bath.tif'))

# load 5km grid
grid_5km <- read_sf(here('GIS_layers', 'fivekm_grid_polys_shore_lamb.shp'))
```

## Transform data

* Based on a filtering check in Excel, I didn't see any signs flipped for latitude and longitude. Points out of the expected range were due to 0's in one or both coordinate fields, so I removed them.
* Create pot strings from set and up points (create lines from points)

```{r transform-logbook-data}

logbook_df <- logbook_df %>% 
  mutate(
    # transform date to date type
    DetailDate = mdy(DetailDate),
    # if longitude or latitude of set or up points are out of range, then convert to NA
    SetLatDec = ifelse(SetLatDec < 1,  NA, SetLatDec),
    UpLatDec  = ifelse(UpLatDec  < 1,  NA, UpLatDec),
    SetLonDec = ifelse(SetLonDec > -1, NA, SetLonDec),
    UpLonDec  = ifelse(UpLonDec  > -1, NA, UpLonDec),
    # add booleans for whether any or all coordinates related to a string were NA
    all_na_coordinate_flag = is.na(SetLatDec) & is.na(SetLonDec) & is.na(UpLatDec) & is.na(UpLonDec),
    any_na_coordinate_flag = is.na(SetLatDec) | is.na(SetLonDec) | is.na(UpLatDec) | is.na(UpLonDec)
  )

# remove logbook entries using spatial flag and NA longitude or latitude
filtered_logbook_df <- logbook_df %>%
  filter(!is.na(SetLonDec) & !is.na(SetLatDec) & !is.na(UpLonDec) & !is.na(UpLatDec)) %>%
  filter(SpatialFlag == "False")
```

```{r transform-points-to-strings}
# transform set and up points to be in different rows for same logbook entry (each row is a point, with type set or up)
string_df <- filtered_logbook_df %>% 
  pivot_longer(
    c(SetLatDec, SetLonDec, UpLatDec, UpLonDec),
    names_to = c("point_type", ".value"),
    names_pattern = "(.*)(Lat|Lon)Dec",
    values_to = ".value"
  )

# start time for spatial transformations
start_time <- proc.time()

# convert from start and end coordinates to strings
string_df <- string_df %>%
  # remove point type that distinguished up vs. set points, don't need that information to make lines
  dplyr::select(-point_type) %>%
  # convert from doubles to spatial data
  st_as_sf(coords = c("Lon", "Lat"), crs = 4326) %>%
  # group logbook strings by all variables other than coordinate geometry
  group_by(LogID, LogDetailID, CrabPermit, CrabYear, DEP, DocNum, PortCode, DetailDate, Depth, NumPots, SoakTime, EstLbs, AdjLbs, AdjValue, TicketNum) %>%
  # transform sf points from degrees to meters for upcoming length calculations
  st_transform(32610) %>%
  # convert sf points to sf linestring
  summarise(do_union = FALSE) %>% 
  st_cast("LINESTRING") %>% 
  ungroup()

# print time
end_time <- proc.time()
cat((end_time[3] - start_time[3])/60, 'minutes to do spatial conversion') # took 1.3 minutes
```

```{r transform-strings-to-pots}
# start time for spatial transformations
start_time <- proc.time()

# distribute traps along the string (points along line)
trap_sf <- string_df %>%
  # remove strings with no pots
  filter(NumPots != 0) %>%
  # distribute NumPots points along the line
  mutate(trap_locations = pmap(.l = list(NumPots, geometry),
                               .f = function(pots, string) st_line_sample(string, n = pots))) %>%
  # extract coordinates of points along the line into a tibble
  mutate(trap_coordinates = map(.x = trap_locations,
                                 .f = function(traps) as_tibble(st_coordinates(traps)))) %>%
  # unnest coordinates from tibbles to separate rows
  unnest(trap_coordinates) %>%
  # drop linestring information, and L1 created by st_coordinates which isn't needed
  st_set_geometry(NULL) %>%
  dplyr::select(-trap_locations, -L1) %>%
  # convert trap coordinates into simple feature
  st_as_sf(coords = c('X', 'Y'), crs = 32610) %>%
  st_transform(4326)

# add depths from bathymetry, divide by 10 to convert from decimeters to meters
trap_sf$trap_coords_depth_m <- raster::extract(bathymetry, trap_sf) / 10

# end time
end_time <- proc.time()
cat((end_time[3] - start_time[3])/60, 'minutes to do spatial conversion') # took 1.5 minutes

# filter based on depth
filtered_trap_sf <- trap_sf %>%
  filter(trap_coords_depth_m <= 0) %>% # pots must be underwater
  filter(trap_coords_depth_m > -200) # maximum depth of 200 meters underwater
```

Note that my experience with the depth filter is that it removes 614 NA depth, and the < -5000 filter has no impact (minimum depth goes from -1159 to -199, no records were present with less than <- 5000 to remove). I removed the < -5000 filter because Leena also didn't mention it in her paper supplementary.

```{r gridded-summary}

# left off at line 245 in script 1
# need to incorporate m2 code in script 2 https://github.com/jameals/raimbow/blob/master/wdfw/code/OR/OR_logbook_analysis_script2_M1_M2.R

# join to 5km grid
start_time <- proc.time()
test_trap_df <- filtered_trap_sf %>%
  st_transform(st_crs(grid_5km)) %>% # transform to same coordinate reference system as grid
  st_join(grid_5km) %>% # join 5km grid to traps simple features
  st_set_geometry(NULL)
end_time <- proc.time()
cat((end_time[3] - start_time[3])/60, 'minutes to do spatial conversion')
rm(start_time, end_time)

# create gridded summary (this is not complete)
crab_year_start <- 11        # month defines start of crab year
winter_months <- c("November", "December", "January", "February", "March") # determine Winter or Spring-Summer season
test_dcrb_5km_summary_df <- test_trap_df %>%
  mutate(
    year = year(DetailDate),
    month = month(DetailDate, label = TRUE, abbr = FALSE),
    month_numeric = month(DetailDate),
    year_month = paste0(year(DetailDate),"_", substr(ymd(DetailDate), 6, 7)),
    crab_year = ifelse(month_numeric >= crab_year_start, paste0(year, "_", 1+year), paste0(year-1, "_", year)),
    season = as.character(ifelse(month %in% winter_months, "Winter", "Spring-Summer")),
    week_of_year = week(DetailDate),
    day_of_year = yday(DetailDate)
  ) %>%
  group_by(
    # grid cell ID, makes this a gridded summary
    GRID5KM_ID,
    # temporal columns, makes this a monthly summary
    year,
    crab_year,
    year_month,
    season,
    month,
    month_numeric
  ) %>%
  summarise(
    n_pots = n(),
    n_strings = n_distinct(LogDetailID),
    n_unique_vessels = n_distinct(DocNum), # is this equivalent to drvid?
    .groups = "keep"
  ) %>% 
  ungroup()
```

Notes

* It seems suspicious to me there are *exactly* 160,000 strings in the logbook data.
